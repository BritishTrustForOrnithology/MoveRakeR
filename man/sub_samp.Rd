% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sub_samp.R
\name{sub_samp}
\alias{sub_samp}
\alias{retrieve_boots}
\title{Sub-sample telemetry data to a standardised sampling rate}
\usage{
sub_samp(
  data,
  dt = 300,
  Unit = "secs",
  by = NULL,
  tol = 0.4,
  u_tol = NULL,
  l_tol = NULL,
  rateU = dt + (dt * tol),
  rateL = dt - (dt * tol),
  method = c("sequencer", "rounder", "boot_sequencer"),
  nboots = 5,
  seed = NULL,
  split_ratio = 50,
  drop_coarser = TRUE,
  verbose = TRUE
)

retrieve_boots(data, drop_coarser = TRUE, verbose = TRUE)
}
\arguments{
\item{data}{Input data object, with required columns: TagID, DateTime, longitude, latitude.}

\item{dt}{The new sampling rate at which we wish the data to be downgraded to. Numeric value.}

\item{Unit}{The unit of the new sampling rate, fed to function \code{base::difftime} specified as
e.g. 'secs' or 'mins'.}

\item{by}{A by variable also which to work with data at a lower level, e.g. year, migration phase (but currently only used by the boot sampler).}

\item{tol}{Lee-way proportion numeric value - ideally between >0 and <=1 giving a proportional amount to be
added or subtracted from the specified filtering rate \code{dt}. See below.}

\item{rateU}{The upper rate (dt+(dt*tol)) at which to remove fixes as above using the \code{tol} parameter; this is the primary
value for which sub-sampling is carried out.}

\item{rateL}{A lower rate given automatically using \code{tol} as (dt-(dt*tol)).
Occasionally the filter will not remove some rates <= dt due to it's matching nature
therefore after the subsampling, the numeric value given in \code{rateL} will drop rates lower than this value. By default,
\code{rateL} and \code{rateU} are therefore both determined by the tolerance proportional value \code{tol}, to
give a window of lee-way error given that GPS rates rarely sample spot on the rate they are programmed to take measurements at
i.e. due to satellite communications, time to fix errors etc.}

\item{method}{A choice of three, "sequencer", "rounder", or "boot_sequencer" as to how to do the downgrading; sequencing (default) uses
data.table to 'roll' DateTimes to nearest on a sequences of dates at the desired sampling rate from the start of the animal per year.
The second option is to round to the nearest whole DateTime at the desired sampling rate, both being valid options.
The function \code{sub_samp} also calls \code{\link{gap_section}} using the drop = TRUE option to remove orphaned gappy sections
of data (single points) and remove all data beyond the desired rate, e.g. if 300 s was wanted, then any coarse rates
such as 1800 s would be dropped, that is if \code{fun} is set to TRUE. The boot sequencer uses the sequence approach
but selects random fixes within consecutive sections of data t0 to t+1 where (t+1)-t = dt; this
approach should be considered at the beta stage. The boot_sequencer samples with replacement. This
process is currently slow for large datasets, e.g. 10 s to five minutes for data across many months per animal;
that is even with use of \code{base::combn} and tidyverse approaches such as \code{slice_sample()}; here
we vectorised the process comparing pairs of points per section samples. The routine works by selecting points within sections of data at length dt, and a validity check
is needed to make sure fixes are not selecting at the boundaries of neighbouring sections < rateL; requiring an
iterative approach. The result is a listed output of boots, that can be translated back into
\code{Track} data using the function \code{retrieve_boots}. Initial tests for five birds for 100
boots downgrading to 300 s from GPS data with a mixture of 10 s, 300 s and 1800 s rates across five months were in the region of a 1.5 hours on an i5 8GB RAM PC, but
parallel processing has not yet been explored. We would welcome any feedback.}

\item{nboots}{The number of bootstraps to take using the sequence sampler, relevant only for that method.}

\item{seed}{Seed to set for reproducibility using the boot sequencer.}

\item{split_ratio}{A ratio to split up pairs of sequences for the boot sequencer option, to speed up processing inthe
\code{combn} \code{data.table} process assessing validity of pairs. Defaults to 50. Toggling this may help runtime.}

\item{drop_coarser}{Logical to drop the coarser rates than the ones we sub-sample for? Default TRUE.
This is handy to set to FALSE if comparing trip statistics and effects of downgrading if multiple rates per trip are evident.}

\item{verbose}{logical to return detailed messaging.}

\item{tol_u}{A bespoke upper tolerance if the user wants to specify a different value for upper and lower tolerance.}

\item{tol_l}{A bespoke lower tolerance if the user wants to specify a different value for upper and lower tolerance.}

\item{fun}{Logical TRUE or FALSE (default TRUE) to use the \code{\link{gap_section}} function to remove coarser rates, see also 'method' argument.}
}
\value{
Returns the same format data as inputted but a reduced dataset (except for the boot_sequencer - see below)
removing rows of data for each animal faster than the desired rate. The boot sequencer returns a list of row numbers matching
the original dataset, to save memory, stored as attributes to the output under \code{attr(attr(data2, "sub_samp"), "boots")}; these are retrieved and converted
to \code{Track} data using the \code{retrieve_boots} function that adds an additional column of 'boot'
in a stacked dataset across all animals.
}
\description{
The function \code{sub_samp} filters the telemetry data to a given rate specified by the user
so that all positional fixes are at a near-enough standard rate, such as 2 mins, 5 mins, 30 mins.
This reduces sampling bias should tags sample more quickly or more slowly over their deployment,
and is needed for analyses such as some of those used to produce time-static utilisation distributions.
}
\details{
Very often, tags deployed on animals will not always be scheduled to sample at a constant rate
or gaps may occur that render particular fixes unusable given potential for spatio-temporal
sampling biases in further analyses. \code{sub_samp} offers a way to reduce \code{Track},
\code{TrackStack} and \code{TrackMultiStack} class objects to a common temporal denominator
based on the start and end time of each Tag's record. Note, however, this method does not fully interpolate
the deployments to a fixed temporal value, as is done in linear and non-linear (e.g. correlated
random walk) interpolation offered through R packages adehabitat (Calenge 2006) and Crawl (Johnson et al. 2008)
\code{sub_samp} is based on original code from Emiel van Loon at the University of Amsterdam.

Three options are available in \code{sub_samp} for subsampling: "sequencer", "rounder", or "boot_sequencer", see
the argument descriptions for more details. These allow subsampling to the nearest time unit based on
either the start time of the animal (sequencer) or the nearest whole time unit (rounder). The boot method
is a beta method that has been developed to produce simulations so as to
avoid reliance on always selecting the same fixes from the sample - useful for example if an animal traverses
a spatial unit a limited number of times and you want o make the most of the data from within the track.
The function \code{retrieve_boots} can be used to access the bootstrapped GPS data, that are stored as lookup indices
from \code{sub_samp} for memory saving purposes.
}
\examples{

# NOT RUN
ColLon = -3.185
ColLat = 54.045
p4 <- sf::st_crs((paste("+proj=laea +lon_0=", ColLon," +lat_0=", ColLat, " +units=m", sep="")))
data <- yourdata # GPS data with tagID, DateTime, longitude and latitude named columns

indata <- data \%>\% clean_GPS(speedfilt = TRUE, drop = FALSE) \%>\%
  define_trips(method="rect", lls = c(-3.2, 54.0553,-3.1689, 54.0437), p4s = p4)

# test of rounding and sequencing approaches
round_test = sub_samp(indata, dt=300, tol = 0.4, method = "rounder")
seq_test = sub_samp(indata, dt=300, tol = 0.4, method = "sequencer")

# If wanting different upper and lower tolerances, e.g. more strict removal of fixes after the sub-sampling has been carried out
round_test = sub_samp(indata, dt=300, u_tol = 0.4, l_tol = 0.2, method = "rounder")

plot_leaflet(round_test)
plot_leaflet(seq_test)

# ------------------------------------------------------------------- #
# EXAMPLE ISSUES DURING THE PROCESSING
# The function presents two main methods for sub-sampling (also called thinning or downgrading) the data
# to coarser sampling rates: (1) Rounding DateTimes to the nearest rate (+ unit) and (2) A sequence of
# DateTimes from the start of the bird (default).
# Both are valid estimations, but will result in different answers for points they may select
#
# Of note, there is a quirk in the way this operates, with potential for overly strict removal of locations
# with either of the above methods snapping to regular time grids where data are close already to the desired
# downgrading rate. Although mathematically correct, without consideration may be overly reductionist.
#
# consider this example of five minute data, and the user wants to downgrade the data to five minutes,
# given that further data are below this rate in the sequence. But focusing here on the data already near 300 s:

dates <- as.POSIXct(c("2014-05-21 05:37:44","2014-05-21 05:42:31","2014-05-21 05:47:20"), tz = "UTC")
data_test_df <- tibble(DateTime = dates, TagID = 1)
data_test_df <- data_test_df \%>\% mutate(difftime = as.vector(difftime(DateTime, lag(DateTime), units= 'secs')))

# these points are all at slightly less than a 5 min rate but still very valid and near to them.

# -------------------------------------------------------- #
# 1. Rounding to DateTime units

df_subsampled_test <- data_test_df \%>\%
  group_by(TagID) \%>\%
  rename(DateTime_orig = "DateTime") \%>\%
  mutate(DateTime = round_date(DateTime_orig, unit = period(num = 5, units = 'mins')),
         diff = abs(as.vector(difftime(DateTime_orig, DateTime, units = "secs")))
  ) \%>\% group_by(TagID, DateTime) \%>\% slice_min(diff) \%>\%
  group_by(TagID) \%>\%
  mutate(dt = as.vector(difftime(lead(DateTime_orig), DateTime_orig, units = 'secs')) ) # recalculate dt

# This results in the middle point being dropped because the 47 is the closer match to the nearest round
# but the three points seem like valid 5 minute samples, albeit below the intended 300s, but very close
#
# subsample           DateTime               TagID difftime
# <dttm>              <dttm>              <dbl>    <dbl>
#  1 2014-05-21 05:40:00 2014-05-21 05:37:44     1       NA
#  2 2014-05-21 05:45:00 2014-05-21 05:47:20     1      289
#
# -------------------------------------------------------- #
# 2. Sequential 'rolling' in data.table to a sequence

#' # ordinarily this is the approach:
#date_start = min(data_test_df$DateTime,na.rm=TRUE)
#date_end = max(data_test_df$DateTime,na.rm=TRUE)
#date_seq = seq(from=date_start, to=date_end, by= paste(5, 'mins))
#if(date_seq[length(date_seq)] < date_end){  date_seq <- c(date_seq,date_seq[length(date_seq)]+dt) }

# but if we ended up with the the start of the animal being at 05:45:03, this would give sequential 5 min sequences at:
date_seq <- as.POSIXct(c("2014-05-21 05:35:03", "2014-05-21 05:40:03", "2014-05-21 05:45:03"), tz = "UTC")
data_seq <- data.frame(TagID = 1, DateTime = date_seq)

# then rolling to the nearest seq DateTime per TagID:
r1 <- data.table::data.table(data_test_df, key = c("TagID","DateTime"))  # raw data
r1$DateTime_orig <- r1$DateTime
r2 <- data.table::data.table(data_seq, key = c("TagID","DateTime"))  # interpolated data
new <- r1[r2 , list(TagID, DateTime, DateTime_orig) , roll = "nearest", allow.cartesian=TRUE ]

new$diff <- as.vector(difftime(new$DateTime_orig,new$DateTime,units = 'secs'))
new$diff <- ifelse(new$diff < 0, new$diff *-1,new$diff)

# which.min again, similar to above under (1)
select_cols <- c("TagID","DateTime","DateTime_orig","diff")
new2 <- data.table::setDT(new)[, .SD[which.min(diff)], by = .(DateTime_orig)][, ..select_cols]

# and so forth
data.table::setkey(new2, TagID, DateTime_orig)
new2[,dt:= (difftime(DateTime_orig, data.table::shift(DateTime_orig,1,type="lag"),unit = "secs")), by = "TagID"]
new2$dt <- c(as.vector(new2$dt)[-1],NA)
new2$dt <- ifelse(is.na(new2$dt),0,new2$dt)

#Key: <DateTime_orig>
#  DateTime       DateTime_orig  diff    dt
# <POSc>              <POSc> <num> <num>
#  1: 2014-05-21 05:40:03 2014-05-21 05:37:44   139   576
#  2: 2014-05-21 05:45:03 2014-05-21 05:47:20   137     0

# -------------------------------------------------------- #
# To get around this, in the sub_samp function, we therefore revisit the original data
# and look for potential real points that could sit in between consecutive fixes that would meet a
# minimum threshold difference between the fixes informed by dt-(dt*tol)
# -------------------------------------------------------- #

# Ultimately, interpolation methods may be more useful depending on further analytical uses.

sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder")

##########################################################################################
dates <- c("2014-05-21 05:37:44","2014-05-21 05:42:31","2014-05-21 05:47:20",
           "2014-05-21 03:32:01", "2014-05-21 03:32:22","2014-05-21 05:36:21",
           "2014-05-21 05:36:40", "2014-05-21 05:37:01", "2014-05-21 05:37:19",
           "2014-05-21 06:37:44", "2014-05-21 07:38:34","2014-05-21 07:38:54")

data_test_df <- data.frame(DateTime = as.POSIXct(c(dates), tz = "UTC"), TagID = 1, difftime = NA) \%>\%
  arrange(DateTime) \%>\%
  mutate(difftime = as.vector(difftime(DateTime, lag(DateTime), units= 'secs')))
data_test_df$test <- "blah" # addtional column in supply data

# testing 3600 s for rounder and sequencer, with drop_coarser TRUE/FALSE
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# 300 s
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "sequencer", drop_coarser = TRUE)

# 20 s
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# 1800 s: Behaviour when you are trying to round to a rate that isn't possible...??
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# but....if not dropping coarser, and the case is true where we (after filtering)
# have no valid data for the rates BUT you want to keep all data COARSER than that
# we should...do so? This therefore acts as a filter taking out anything less than 1800+tol

sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = FALSE)

## back to real data tests
data <- yourdata
x1800 <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = TRUE)
x3600 <- sub_samp(data, dt = 3600, tol = 0.4, method = "sequencer", drop_coarser = TRUE)
x300 <- sub_samp(data = data, dt = 300, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# -------------------------------------------------------- #
# 3. Boot sequencer
# produce 'realisations' of the data randomly sampling
# This can be lengthy if say you have lots of 10 s data and wanting to go to e.g. 1800!
# So that's a lot of potential combinations of pairs of points to assess validity in the current approach.

x1800_boot <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "boot_sequencer", drop_coarser = TRUE)
x1800_actual_boots <- retrieve_boots(x1800_boot) # use of retrieve-boots function to access boots (stored from the sub_samp function as an index lookup to save memory)

data_test_df <- data_test_df \%>\% mutate(difftime = as.vector(difftime(DateTime, lag(DateTime), units= 'secs')))

new3 = sub_samp(data, dt=7200, tol = 0.2, method = "boot_sequencer", nboots = 10, seed = 1)
new = retrieve_boots(new3)
plot_leaflet(data = data, plotby = "boot") # check visualisation

}
\references{
Calenge, C. (2006) The package adehabitat for the R software: tool for the analysis
of space and habitat use by animals. \emph{Ecological Modelling}, \strong{197}, 516-519

Johnson, D.S., London, J.M., Lea, M.-A. & Durban, J.W. (2008) Continuous-time correlated random
walk model for animal telemetry data. \emph{Ecology}, \strong{89}, 1208-1215. doi:10.1890/07-1032.1.

Johnson, D.S. & London, J.M. (2018) crawl: an R package for fitting continuous-time
correlated random walk models to animal movement data. Zenodo.
\url{https://doi.org/10.5281/zenodo.596464}
}
\seealso{
\link{clean_GPS}, \link{gap_section}, \link{Track2move}
}
