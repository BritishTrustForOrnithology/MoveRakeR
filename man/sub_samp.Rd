% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sub_samp.R
\name{sub_samp}
\alias{sub_samp}
\alias{retrieve_boots}
\title{Sub-sample telemetry data to a standardised sampling rate}
\usage{
sub_samp(
  data,
  dt = 300,
  Unit = "secs",
  by = NULL,
  tol = 0.2,
  u_tol = NULL,
  l_tol = NULL,
  rateU = NULL,
  rateL = NULL,
  method = c("sequencer", "rounder", "boot_sequencer", "boot_rate"),
  strict = FALSE,
  blind = FALSE,
  n_boot = 5,
  seed = NULL,
  drop_coarser = TRUE,
  verbose = TRUE,
  verbose_repair = FALSE
)

retrieve_boots(data, animal = NULL, include_coarser = FALSE, verbose = TRUE)
}
\arguments{
\item{data}{Input data object, with required columns: TagID, DateTime, longitude, latitude.}

\item{dt}{The new sampling rate at which we wish the data to be downgraded to. Numeric value.}

\item{Unit}{The unit of the new sampling rate, fed to function \code{base::difftime} specified as
e.g. 'secs' or 'mins'.}

\item{by}{A by variable also which to work with data at a lower level, e.g. year, migration phase (but currently only used by the boot sampler).}

\item{tol}{Lee-way proportion numeric value - ideally between >0 and <=1 giving a proportional amount to be
added or subtracted from the specified filtering rate \code{dt}. See below.}

\item{rateU}{The upper rate (dt+(dt*tol)) at which to remove fixes as above using the \code{tol} parameter; this is the primary
value for which sub-sampling is carried out in all methods; see rateL for more details of calculation with tol arguments.}

\item{rateL}{A lower rate given automatically using \code{tol} as (dt-(dt*tol)).
For the methods 'sequencer' and 'rounder', occasionally the filter will not remove some rates <= dt due to it's matching nature
therefore after the subsampling, the numeric value given in \code{rateL} will drop rates lower than this value. For the 'boot_sequencer'
and 'boot_rate' methods, rateL is used as a minimum value to search for within segment windows of length rateU.
By default, \code{rateL} and \code{rateU} are therefore both determined by the tolerance proportional value \code{tol}, to
give a window of lee-way error given that GPS rates rarely sample spot on the rate they are programmed to take measurements at
i.e. due to satellite communications, time to fix errors etc. However rateL and rateU can also be determined by
bespoke l_tol or u_tol parameters that override tol, which are in turn also overridden if the user specifies
a direct raeL or rateU. The hierarchy of rateL and rateU determination in order of
increasing priority are are thus via tol, u_tol or l_tol and finally via the rateL or rateU provision directly. By default
rateU and rateL are NULL, and so tol values are used.}

\item{method}{A choice of four, "sequencer", "rounder", "boot_sequencer", or "boot_rate" as to how to do the downgrading:
(1) The sequencer is the default method that \code{sub_samp()} uses, with a
data.table used to 'roll' DateTimes to nearest on a sequences of dates at the desired sampling rate from the start of the animal per year.
(2) The second option is to round to the nearest whole DateTime at the desired sampling rate, both being valid options.
The function \code{sub_samp} also calls \code{\link{gap_section}} using the drop = TRUE option to remove isolated gappy sections
of data (single points) and remove all data beyond the desired rate, e.g. if 300 s was wanted, then any coarse rates
such as 1800 s would be dropped, that is if \code{fun} is set to TRUE.
(3) The boot sequencer uses the sequence approach as above
but selects random fixes within consecutive sections of data t0 to t+1 where (t+1)-t = dt.
The boot_sequencer samples with replacement, and searches within 'segments' of points within windows length rateU.
This window search operates across TagID/'VAR'/gapsec levels with 'VAR' being the potential by variable that can be included in the hierarchy.
This process of course slower for large datasets, e.g. 10 s to five minutes for data across many months per animal. The sequencer
also has a quirk if used with default settings (blind = TRUE, strict = FALSE), in that across windows of the data,
there will naturally be only certain GPS fixes that can be sampled from the segment windows, dependent on the rateL and rateU conditions used.
This will 'lock in' the same fixes sampled on the same bootstraps each time. Options are available to relax this condition
through setting the 'blind' argument to TRUE, which will forget the previous point selection in the sequence. If the 'strict'
argument is also set to TRUE, then any fixes too close to a previous segment selection will be dropped. With 'strict' as FALSE (default)
these points will not be dropped and any gaps of too lengthy time spans where rateU is violated will be checked for other points to insert.
A more conservative but still quite strict approach is to lower the lower tolerance 'l_tol' value, e.g. to 0.4 from 0.2 overall 'tol'
so that some variation within the sequence of fixes is allowed, but this is a trade-off as the lower l_tol specified will
increasingly deviate away from the intended overall 'dt' rate downgraded to.
Another option is to use the fourth method option:
(4) This introduces another level into the hierarchy so that sequences are performed within TagID/'VAR'/rate/gapsec.
This uses the \code{assign_rates()} function \strong{with defaults from that function}, to allow strings of fixes
along which the sequence of bootstrapping is performed to occur within rate changes of the tag, which is useful if there
are multiple rates used. This will begin the search of random points at each rate switch to break that 'locked in' pattern
of samples across bootstraps. However, this will also result in possible no valid fixes being found at those boundaries.
\strong{A warning here}: If your data is on the scale of one fix every day, then the 'boot_rate' approach will not work as
the defaults are used within \code{assign_rates()}, which guesses rates up to three hours.#'
The result of the two boot methods is a listed output of boots, that can be translated back into
\code{Track} data using the function \code{retrieve_boots}.}

\item{strict}{Logical, defaults to FALSE. Determines whether the function drops out the fixes that are too fast and violate
the rateL lower bound as determined by the tol or l_tol arguments. In the boot methods, frequently, fixes cannot be sampled to meet
both rateL and rateU, particularly at coarser rates from slightly less coarser rates, where fewer fixes are available fore resampling.
This of course depends on the tolerance parameters used too. That said a situation can also arise if for example
one sampling segment has an issue finding a fix skipping it, which forces the next segment along to randomly pick from any of the fixes
in that second segment, which may be further forward in time > rateU between remaining sampled fixes; this is because
essentially that second segment has to start the search off again considering all fixes.
This can give 'holes' in the data, but these on occasion can be back-filled (repaired) by going back to the original data to check for valid points, and if
strict = FALSE, this will happen with 1:n fixes randomly inserted, where n is the maximum possible on that slice of the data.}

\item{blind}{Logical, default FALSE. Following on from the strict argument, if blind is TRUE, all segments sample 'blindly'
where within segment t, any previous GPS random pick of a point in the segment t-1 is forgotten. This allows a full true randomisation
but will violate rateL and rateU conditions. Therefore strict arguments should be considered alongside. If strict is TRUE as well as blind being TRUE,
the result is removal of fixes violating rateL but keeping 'holes' in the data from rateU segment to segment quirks (see strict argument).
If blind is TRUE but strict is FALSE, then faster sampled points are retained, but attempts are made to put potentially missing fixes into data 'holes'.}

\item{seed}{Seed to set for reproducibility using the boot sequencer.}

\item{drop_coarser}{Logical to drop the coarser rates than the ones we sub-sample for? Default TRUE.
This is handy to set to FALSE if comparing trip statistics and effects of downgrading if multiple rates per trip are evident.}

\item{verbose}{logical to return detailed messaging.}

\item{verbose_repair}{Logical defaulting to FALSE. This turns on detailed messaging for the routine that
attempts to 'repair' the randomly sub-sampled data after the "boot_sequencer" or "boot_rate" methods have been used.}

\item{tol_u}{A bespoke upper tolerance if the user wants to specify a different value for upper and lower tolerance.}

\item{tol_l}{A bespoke lower tolerance if the user wants to specify a different value for upper and lower tolerance.}

\item{fun}{Logical TRUE or FALSE (default TRUE) to use the \code{\link{gap_section}} function to remove coarser rates, see also 'method' argument.}

\item{nboots}{The number of bootstraps to take using the sequence sampler, relevant only for that method.}
}
\value{
Returns the same format data as inputted but a reduced dataset (except for the boot_sequencer - see below)
removing rows of data for each animal faster than the desired rate. The boot sequencer returns a list of row numbers matching
the original dataset, to save memory, stored as attributes to the output under \code{attr(attr(data2, "sub_samp"), "boots")}; these are retrieved and converted
to \code{Track} data using the \code{retrieve_boots} function that adds an additional column of 'boot'
in a stacked dataset across all animals.
}
\description{
The function \code{sub_samp} filters the telemetry data to a given rate specified by the user
so that all positional fixes are at a near-enough standard rate, such as 2 mins, 5 mins, 30 mins.
This reduces sampling bias should tags sample more quickly or more slowly over their deployment,
and is needed for analyses such as some of those used to produce time-static utilisation distributions.
}
\details{
Very often, tags deployed on animals will not always be scheduled to sample at a constant rate
or gaps may occur that render particular fixes unusable given potential for spatio-temporal
sampling biases in further analyses. \code{sub_samp} offers a way to reduce \code{Track},
\code{TrackStack} and \code{TrackMultiStack} class objects to a common temporal denominator
based on the start and end time of each Tag's record. Note, however, this method does not fully interpolate
the deployments to a fixed temporal value, as is done in linear and non-linear (e.g. correlated
random walk) interpolation offered through R packages adehabitat (Calenge 2006) and Crawl (Johnson et al. 2008)
\code{sub_samp} has its early origins from original code from Emiel van Loon at the University of Amsterdam.

Four options are available in \code{sub_samp} for subsampling: "sequencer", "rounder", "boot_sequencer" or "boot_rate", see
the argument descriptions for more details. These allow subsampling to the nearest time unit based on
either the start time of the animal (sequencer) or the nearest whole time unit (rounder). These methods also handle
situations where two competing fixes may be available to be rounded to the same time, which can result in holes in the data if
only one valid fix is picked. The default of \code{sub_samp()} is to preserve a true downgraded sample, and so the "rounder"
and "sequencer" methods (as also with the boot methods) make an attempt to randomly fill in potentially missing fixes in such cases
via a final sweep of the data.

The boot methods have been developed to produce simulations so as to
avoid reliance on always selecting the same fixes from the sample - useful for example if an animal traverses
a spatial unit a limited number of times and you want o make the most of the data from within the track.
The function \code{retrieve_boots} can be used to access the bootstrapped GPS data, that are stored as lookup indices
from \code{sub_samp} for memory saving purposes.

Regarding the bootstrap methods, these are complex and tricky to arrive at a true random sample as it depends
ultimately on the structure of the data underlying. The aim is to preserve the linear temporal
structure of the original track as much as possible, just at a downgraded rate. In doing so, the \code{sub_samp()} function caters for as made of these
data quirks as possible. In particular, a key issue is when a sequence of fixes is 'locked in', for example when a switch of
rates of the tag occurs, or when there are just too few samples to choose from to meet a desired acceptable
lower and upper bound around the target downgrade rate. As in other \code{MoveRakeR} functions, tolerance values
are allowed to vary this bound around the rate required resulting in a lower 'rateL' and upper 'rateU' around the original 'dt'
sampling rate. Widening this gap will bring more GPS fixes into play within the samplers, but at expense of deviating from the targetted downgrade rate.
The samplers operate via random search windows where one fixe is randomly picked in a 'segment' window along the track where the segment length is rateU.
The ambition is that sampled fixes meet both rateL and rateU condition, but may not always possible for every segment and boot.
Some flexibility is therefore provided in \code{sub_samp()} to vary the method used. The primary "boot_sequencer" approach is similar to the
"sequencer" method in sampling within strings of points, with the first fixes' DateTime used to start the sampling off.
The string of points considered for segmentation is thus a nested hierarchy of: TagID/(optional by variable)/gapsection. Another level is introduced in the second
boot method "boot_rate" that still uses a sequence from the first DateTime in the valid ctring of points,
but using: TagID/(optional by variable)/rate/gapsection to begin the segment randomisation process. Strictness in how the samplers treats fixes that are too close
together after the sampling is finished, as well as a repairing process to insert potentially valid fixes back in if holes in the data arise.
Finally, the sampling can also be done 'blindly' without remembering any fixes in the sequence, but at a cost
of violating the rateL/rateU conditions between sampled fixes.
}
\examples{

# NOT RUN
ColLon = -3.185
ColLat = 54.045
p4 <- sf::st_crs((paste("+proj=laea +lon_0=", ColLon," +lat_0=", ColLat, " +units=m", sep="")))
data <- yourdata # GPS data with tagID, DateTime, longitude and latitude named columns

indata <- data \%>\% clean_GPS(speedfilt = TRUE, drop = FALSE) \%>\%
  define_trips(method="rect", lls = c(-3.2, 54.0553,-3.1689, 54.0437), p4s = p4)

# test of rounding and sequencing approaches
round_test = sub_samp(indata, dt=300, tol = 0.4, method = "rounder")
seq_test = sub_samp(indata, dt=300, tol = 0.4, method = "sequencer")

# If wanting different upper and lower tolerances, e.g. more strict removal of fixes after the sub-sampling has been carried out
round_test = sub_samp(indata, dt=300, u_tol = 0.4, l_tol = 0.2, method = "rounder")

plot_leaflet(round_test)
plot_leaflet(seq_test)

# ------------------------------------------------------------------- #
# EXAMPLE ISSUES DURING THE PROCESSING
# The function presents two main methods for sub-sampling (also called thinning or downgrading) the data
# to coarser sampling rates: (1) Rounding DateTimes to the nearest rate (+ unit) and (2) A sequence of
# DateTimes from the start of the bird (default).
# Both are valid estimations, but will result in different answers for points they may select
#
# Of note, there is a quirk in the way this operates, with potential for overly strict removal of locations
# with either of the above methods snapping to regular time grids where data are close already to the desired
# downgrading rate. Although mathematically correct, without consideration may be overly reductionist.
#
# consider this example of five minute data, and the user wants to downgrade the data to five minutes,
# given that further data are below this rate in the sequence. But focusing here on the data already near 300 s:

dates <- as.POSIXct(c("2014-05-21 05:37:44","2014-05-21 05:42:31","2014-05-21 05:47:20"), tz = "UTC")
data_test_df <- tibble(DateTime = dates, TagID = 1)
data_test_df <- data_test_df \%>\% mutate(difftime = as.vector(difftime(DateTime, lag(DateTime), units= 'secs')))

# these points are all at slightly less than a 5 min rate but still very valid and near to them.

# -------------------------------------------------------- #
# 1. Rounding to DateTime units

df_subsampled_test <- data_test_df \%>\%
  group_by(TagID) \%>\%
  rename(DateTime_orig = "DateTime") \%>\%
  mutate(DateTime = round_date(DateTime_orig, unit = period(num = 5, units = 'mins')),
         diff = abs(as.vector(difftime(DateTime_orig, DateTime, units = "secs")))
  ) \%>\% group_by(TagID, DateTime) \%>\% slice_min(diff) \%>\%
  group_by(TagID) \%>\%
  mutate(dt = as.vector(difftime(lead(DateTime_orig), DateTime_orig, units = 'secs')) ) # recalculate dt

# This results in the middle point being dropped because the 47 is the closer match to the nearest round
# but the three points seem like valid 5 minute samples, albeit below the intended 300s, but very close
#
# subsample           DateTime               TagID difftime
# <dttm>              <dttm>              <dbl>    <dbl>
#  1 2014-05-21 05:40:00 2014-05-21 05:37:44     1       NA
#  2 2014-05-21 05:45:00 2014-05-21 05:47:20     1      289
#
# -------------------------------------------------------- #
# 2. Sequential 'rolling' in data.table to a sequence

#' # ordinarily this is the approach:
#date_start = min(data_test_df$DateTime,na.rm=TRUE)
#date_end = max(data_test_df$DateTime,na.rm=TRUE)
#date_seq = seq(from=date_start, to=date_end, by= paste(5, 'mins))
#if(date_seq[length(date_seq)] < date_end){  date_seq <- c(date_seq,date_seq[length(date_seq)]+dt) }

# but if we ended up with the the start of the animal being at 05:45:03, this would give sequential 5 min sequences at:
date_seq <- as.POSIXct(c("2014-05-21 05:35:03", "2014-05-21 05:40:03", "2014-05-21 05:45:03"), tz = "UTC")
data_seq <- data.frame(TagID = 1, DateTime = date_seq)

# then rolling to the nearest seq DateTime per TagID:
r1 <- data.table::data.table(data_test_df, key = c("TagID","DateTime"))  # raw data
r1$DateTime_orig <- r1$DateTime
r2 <- data.table::data.table(data_seq, key = c("TagID","DateTime"))  # interpolated data
new <- r1[r2 , list(TagID, DateTime, DateTime_orig) , roll = "nearest", allow.cartesian=TRUE ]

new$diff <- as.vector(difftime(new$DateTime_orig,new$DateTime,units = 'secs'))
new$diff <- ifelse(new$diff < 0, new$diff *-1,new$diff)

# which.min again, similar to above under (1)
select_cols <- c("TagID","DateTime","DateTime_orig","diff")
new2 <- data.table::setDT(new)[, .SD[which.min(diff)], by = .(DateTime_orig)][, ..select_cols]

# and so forth
data.table::setkey(new2, TagID, DateTime_orig)
new2[,dt:= (difftime(DateTime_orig, data.table::shift(DateTime_orig,1,type="lag"),unit = "secs")), by = "TagID"]
new2$dt <- c(as.vector(new2$dt)[-1],NA)
new2$dt <- ifelse(is.na(new2$dt),0,new2$dt)

#Key: <DateTime_orig>
#  DateTime       DateTime_orig  diff    dt
# <POSc>              <POSc> <num> <num>
#  1: 2014-05-21 05:40:03 2014-05-21 05:37:44   139   576
#  2: 2014-05-21 05:45:03 2014-05-21 05:47:20   137     0

# -------------------------------------------------------- #
# To get around the rounder issue, in the sub_samp() function, we therefore revisit the original data
# and look for potential real points that could sit in between consecutive fixes that would meet a
# minimum threshold difference between the fixes informed by dt-(dt*tol)
# -------------------------------------------------------- #

# Ultimately, interpolation methods may be more useful depending on further analytical uses.

sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder")

#Using method: rounder
#Computed rateL from tolerance: 180
#Computed rateU from tolerance: 420
#---- rounding process ----
#  ---- overly strict point removal checker ----
#  TagID difftime            DateTime  dt rn
#1     1       NA 2014-05-21 05:37:44 287  1
#2     1      287 2014-05-21 05:42:31 289  2 <------ better
#3     1       NA 2014-05-21 05:47:20  NA  3

##########################################################################################
dates <- c("2014-05-21 05:37:44","2014-05-21 05:42:31","2014-05-21 05:47:20",
           "2014-05-21 03:32:01", "2014-05-21 03:32:22","2014-05-21 05:36:21",
           "2014-05-21 05:36:40", "2014-05-21 05:37:01", "2014-05-21 05:37:19",
           "2014-05-21 06:37:44", "2014-05-21 07:38:34","2014-05-21 07:38:54")

data_test_df <- data.frame(DateTime = as.POSIXct(c(dates), tz = "UTC"), TagID = 1, difftime = NA) \%>\%
  arrange(DateTime) \%>\%
  mutate(difftime = as.vector(difftime(DateTime, lag(DateTime), units= 'secs')))
data_test_df$test <- "blah" # addtional column in supply data

# testing 3600 s for rounder and sequencer, with drop_coarser TRUE/FALSE
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 3600, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# 300 s
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 300, tol = 0.4, method = "sequencer", drop_coarser = TRUE)

# 20 s
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "sequencer", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 20, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# 1800 s: Behaviour when you are trying to round to a rate that isn't possible...??
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "rounder", drop_coarser = TRUE)
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# but....if not dropping coarser, and the case is true where we (after filtering)
# have no valid data for the rates BUT you want to keep all data COARSER than that
# we should...do so? This therefore acts as a filter taking out anything less than 1800+tol

sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "rounder", drop_coarser = FALSE)
sub_samp(data = data_test_df, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = FALSE)

## back to real data tests
data <- yourdata
x1800 <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "sequencer", drop_coarser = TRUE)
x3600 <- sub_samp(data, dt = 3600, tol = 0.4, method = "sequencer", drop_coarser = TRUE)
x300 <- sub_samp(data = data, dt = 300, tol = 0.2, method = "sequencer", drop_coarser = TRUE)

# -------------------------------------------------------- #
# 3. Boot sequencers

# produce 'realisations' of the data randomly sampling
# This can be quite lengthy if say you have lots of animals, lots of 10 s data and wanting to go to e.g. 1800 s
# We have stuck to use of loops in the sub_samp() function as after much testing, these were actually
# some of the fastest approaches.

# The two boot options:
# ------------------ #
# (a) "boot_sequencer" using TagID/(by variable)/gapsec/segment approach:

x1800_boot <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "boot_sequencer")
boots_x1800 <- retrieve_boots(x1800_boot) # use of retrieve-boots function to access boots (stored from the sub_samp function as an index lookup to save memory)

RakeRvis::RakeRvis(data = boots_x1800) # check visualisation, plotting 'by' boot in the app or...
plot_leaflet(data = boots_x1800, plotby = "boot")

# example using random points in all segments, and patching potential fixes in that violate rateL conditions after:
x1800_boot_blind <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "boot_sequencer", blind = TRUE)

#or being strict and dropping out any fixes that violate rateL without patching any fixes back in
x1800_boot_blind <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "boot_sequencer", blind = TRUE, strict = TRUE)

# example using a more leniant lower tolerance value, preserving the sequence of point randomisation
x1800_boot_ltol <- sub_samp(data = data, dt = 1800, tol = 0.2, l_tol = 0.4, method = "boot_sequencer", blind = FALSE, strict = FALSE)

# ------------------ #
(b) "boot_rate" using TagID/(by variable)/rate/gapsec/segment approach:

x1800_boot_rate <- sub_samp(data = data, dt = 1800, tol = 0.2, method = "boot_rate", drop_coarser = TRUE)
boots_x1800_rate <- retrieve_boots(x1800_boot)

}
\references{
Calenge, C. (2006) The package adehabitat for the R software: tool for the analysis
of space and habitat use by animals. \emph{Ecological Modelling}, \strong{197}, 516-519

Johnson, D.S., London, J.M., Lea, M.-A. & Durban, J.W. (2008) Continuous-time correlated random
walk model for animal telemetry data. \emph{Ecology}, \strong{89}, 1208-1215. doi:10.1890/07-1032.1.

Johnson, D.S. & London, J.M. (2018) crawl: an R package for fitting continuous-time
correlated random walk models to animal movement data. Zenodo.
\url{https://doi.org/10.5281/zenodo.596464}
}
\seealso{
\link{clean_GPS}, \link{gap_section}, \link{Track2move}, \link{progress_estimated2}
}
